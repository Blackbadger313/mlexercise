{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZ9jORg0wAn-",
        "outputId": "b1bf4bbd-aea2-4388-9067-f0d938a468ab"
      },
      "outputs": [],
      "source": [
        "from sklearn import linear_model\n",
        "reg = linear_model.Lasso(alpha=0.1)\n",
        "reg.fit([[0, 0], [1, 1]], [0, 1])\n",
        "\n",
        "reg.predict([[1, 1]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zX8iX0d1Xxb"
      },
      "source": [
        "Lasso and Elastic Net for Sparse Signals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "d6jBBuKZ1Xci",
        "outputId": "1558acd9-07d3-442f-fff0-5a16f42bad38"
      },
      "outputs": [],
      "source": [
        "print(__doc__)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# #############################################################################\n",
        "# Generate some sparse data to play with\n",
        "np.random.seed(42)\n",
        "\n",
        "n_samples, n_features = 50, 100\n",
        "X = np.random.randn(n_samples, n_features)\n",
        "\n",
        "# Decreasing coef w. alternated signs for visualization\n",
        "idx = np.arange(n_features)\n",
        "coef = (-1) ** idx * np.exp(-idx / 10)\n",
        "coef[10:] = 0  # sparsify coef\n",
        "y = np.dot(X, coef)\n",
        "\n",
        "# Add noise\n",
        "y += 0.01 * np.random.normal(size=n_samples)\n",
        "\n",
        "# Split data in train set and test set\n",
        "n_samples = X.shape[0]\n",
        "X_train, y_train = X[:n_samples // 2], y[:n_samples // 2]\n",
        "X_test, y_test = X[n_samples // 2:], y[n_samples // 2:]\n",
        "\n",
        "# #############################################################################\n",
        "# Lasso\n",
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "alpha = 0.1\n",
        "lasso = Lasso(alpha=alpha)\n",
        "\n",
        "y_pred_lasso = lasso.fit(X_train, y_train).predict(X_test)\n",
        "r2_score_lasso = r2_score(y_test, y_pred_lasso)\n",
        "print(lasso)\n",
        "print(\"r^2 on test data : %f\" % r2_score_lasso)\n",
        "\n",
        "# #############################################################################\n",
        "# ElasticNet\n",
        "from sklearn.linear_model import ElasticNet\n",
        "\n",
        "enet = ElasticNet(alpha=alpha, l1_ratio=0.7)\n",
        "\n",
        "y_pred_enet = enet.fit(X_train, y_train).predict(X_test)\n",
        "r2_score_enet = r2_score(y_test, y_pred_enet)\n",
        "print(enet)\n",
        "print(\"r^2 on test data : %f\" % r2_score_enet)\n",
        "\n",
        "m, s, _ = plt.stem(np.where(enet.coef_)[0], enet.coef_[enet.coef_ != 0],\n",
        "                   markerfmt='x', label='Elastic net coefficients',\n",
        "                   use_line_collection=True)\n",
        "plt.setp([m, s], color=\"#2ca02c\")\n",
        "m, s, _ = plt.stem(np.where(lasso.coef_)[0], lasso.coef_[lasso.coef_ != 0],\n",
        "                   markerfmt='x', label='Lasso coefficients',\n",
        "                   use_line_collection=True)\n",
        "plt.setp([m, s], color='#ff7f0e')\n",
        "plt.stem(np.where(coef)[0], coef[coef != 0], label='true coefficients',\n",
        "         markerfmt='bx', use_line_collection=True)\n",
        "\n",
        "plt.legend(loc='best')\n",
        "plt.title(\"Lasso $R^2$: %.3f, Elastic Net $R^2$: %.3f\"\n",
        "          % (r2_score_lasso, r2_score_enet))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zbjWEKT1dhF"
      },
      "source": [
        "Compressive sensing: tomography reconstruction with L1 prior (Lasso)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "ULANIyGW1d-O",
        "outputId": "8ba22528-09fa-4c37-d938-b6157b6c5819"
      },
      "outputs": [],
      "source": [
        "print(__doc__)\n",
        "\n",
        "import numpy as np\n",
        "from scipy import sparse\n",
        "from scipy import ndimage\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import Ridge\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def _weights(x, dx=1, orig=0):\n",
        "    x = np.ravel(x)\n",
        "    floor_x = np.floor((x - orig) / dx).astype(np.int64)\n",
        "    alpha = (x - orig - floor_x * dx) / dx\n",
        "    return np.hstack((floor_x, floor_x + 1)), np.hstack((1 - alpha, alpha))\n",
        "\n",
        "\n",
        "def _generate_center_coordinates(l_x):\n",
        "    X, Y = np.mgrid[:l_x, :l_x].astype(np.float64)\n",
        "    center = l_x / 2.\n",
        "    X += 0.5 - center\n",
        "    Y += 0.5 - center\n",
        "    return X, Y\n",
        "\n",
        "\n",
        "def build_projection_operator(l_x, n_dir):\n",
        "    \"\"\" Compute the tomography design matrix.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "    l_x : int\n",
        "        linear size of image array\n",
        "\n",
        "    n_dir : int\n",
        "        number of angles at which projections are acquired.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    p : sparse matrix of shape (n_dir l_x, l_x**2)\n",
        "    \"\"\"\n",
        "    X, Y = _generate_center_coordinates(l_x)\n",
        "    angles = np.linspace(0, np.pi, n_dir, endpoint=False)\n",
        "    data_inds, weights, camera_inds = [], [], []\n",
        "    data_unravel_indices = np.arange(l_x ** 2)\n",
        "    data_unravel_indices = np.hstack((data_unravel_indices,\n",
        "                                      data_unravel_indices))\n",
        "    for i, angle in enumerate(angles):\n",
        "        Xrot = np.cos(angle) * X - np.sin(angle) * Y\n",
        "        inds, w = _weights(Xrot, dx=1, orig=X.min())\n",
        "        mask = np.logical_and(inds >= 0, inds < l_x)\n",
        "        weights += list(w[mask])\n",
        "        camera_inds += list(inds[mask] + i * l_x)\n",
        "        data_inds += list(data_unravel_indices[mask])\n",
        "    proj_operator = sparse.coo_matrix((weights, (camera_inds, data_inds)))\n",
        "    return proj_operator\n",
        "\n",
        "\n",
        "def generate_synthetic_data():\n",
        "    \"\"\" Synthetic binary data \"\"\"\n",
        "    rs = np.random.RandomState(0)\n",
        "    n_pts = 36\n",
        "    x, y = np.ogrid[0:l, 0:l]\n",
        "    mask_outer = (x - l / 2.) ** 2 + (y - l / 2.) ** 2 < (l / 2.) ** 2\n",
        "    mask = np.zeros((l, l))\n",
        "    points = l * rs.rand(2, n_pts)\n",
        "    mask[(points[0]).astype(int), (points[1]).astype(int)] = 1\n",
        "    mask = ndimage.gaussian_filter(mask, sigma=l / n_pts)\n",
        "    res = np.logical_and(mask > mask.mean(), mask_outer)\n",
        "    return np.logical_xor(res, ndimage.binary_erosion(res))\n",
        "\n",
        "\n",
        "# Generate synthetic images, and projections\n",
        "l = 128\n",
        "proj_operator = build_projection_operator(l, l // 7)\n",
        "data = generate_synthetic_data()\n",
        "proj = proj_operator @ data.ravel()[:, np.newaxis]\n",
        "proj += 0.15 * np.random.randn(*proj.shape)\n",
        "\n",
        "# Reconstruction with L2 (Ridge) penalization\n",
        "rgr_ridge = Ridge(alpha=0.2)\n",
        "rgr_ridge.fit(proj_operator, proj.ravel())\n",
        "rec_l2 = rgr_ridge.coef_.reshape(l, l)\n",
        "\n",
        "# Reconstruction with L1 (Lasso) penalization\n",
        "# the best value of alpha was determined using cross validation\n",
        "# with LassoCV\n",
        "rgr_lasso = Lasso(alpha=0.001)\n",
        "rgr_lasso.fit(proj_operator, proj.ravel())\n",
        "rec_l1 = rgr_lasso.coef_.reshape(l, l)\n",
        "\n",
        "plt.figure(figsize=(8, 3.3))\n",
        "plt.subplot(131)\n",
        "plt.imshow(data, cmap=plt.cm.gray, interpolation='nearest')\n",
        "plt.axis('off')\n",
        "plt.title('original image')\n",
        "plt.subplot(132)\n",
        "plt.imshow(rec_l2, cmap=plt.cm.gray, interpolation='nearest')\n",
        "plt.title('L2 penalization')\n",
        "plt.axis('off')\n",
        "plt.subplot(133)\n",
        "plt.imshow(rec_l1, cmap=plt.cm.gray, interpolation='nearest')\n",
        "plt.title('L1 penalization')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplots_adjust(hspace=0.01, wspace=0.01, top=1, bottom=0, left=0,\n",
        "                    right=1)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-H7W4zb4yqx"
      },
      "source": [
        "Common pitfalls in the interpretation of coefficients of linear models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_7xvJz_4zV8",
        "outputId": "45c6d7ea-c1e0-4b6b-9108-6aa32c722b7c"
      },
      "outputs": [],
      "source": [
        "print(__doc__)\n",
        "\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Djnzxm3044z8"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "survey = fetch_openml(data_id=534, as_frame=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "B78qALvx474h",
        "outputId": "89555f54-7eab-466f-9879-29fc5e7a77a3"
      },
      "outputs": [],
      "source": [
        "X = survey.data[survey.feature_names]\n",
        "X.describe(include=\"all\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "SgEv6h_W4_w7",
        "outputId": "700cb4b5-b4f3-47a6-8782-65848fe4c256"
      },
      "outputs": [],
      "source": [
        "X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSjS7s4y5Cb7",
        "outputId": "7caaa4c6-364e-4013-8202-8b268c0d50c0"
      },
      "outputs": [],
      "source": [
        "y = survey.target.values.ravel()\n",
        "survey.target.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGxDDxNQ5FRT"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 726
        },
        "id": "9qBaUyzZ5Hwb",
        "outputId": "ab550504-b0ed-42c8-dd1d-51305a27d634"
      },
      "outputs": [],
      "source": [
        "train_dataset = X_train.copy()\n",
        "train_dataset.insert(0, \"WAGE\", y_train)\n",
        "_ = sns.pairplot(train_dataset, kind=\"reg\", diag_kind=\"kde\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pvWc_Sa5bFX"
      },
      "source": [
        "The machine-learning pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlbbNVO05bzF",
        "outputId": "6a0a0389-9c97-4f70-c06e-eabfac513885"
      },
      "outputs": [],
      "source": [
        "survey.data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eG6NeFd5430F"
      },
      "source": [
        "Lasso model selection: Cross-Validation / AIC / BIC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 915
        },
        "id": "s4rl6C-J42vC",
        "outputId": "e0839cf3-4884-4ea7-e344-2c0bccda63a5"
      },
      "outputs": [],
      "source": [
        "print(__doc__)\n",
        "\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.linear_model import LassoCV, LassoLarsCV, LassoLarsIC\n",
        "from sklearn import datasets\n",
        "\n",
        "# This is to avoid division by zero while doing np.log10\n",
        "EPSILON = 1e-4\n",
        "\n",
        "X, y = datasets.load_diabetes(return_X_y=True)\n",
        "\n",
        "rng = np.random.RandomState(42)\n",
        "X = np.c_[X, rng.randn(X.shape[0], 14)]  # add some bad features\n",
        "\n",
        "# normalize data as done by Lars to allow for comparison\n",
        "X /= np.sqrt(np.sum(X ** 2, axis=0))\n",
        "\n",
        "# #############################################################################\n",
        "# LassoLarsIC: least angle regression with BIC/AIC criterion\n",
        "\n",
        "model_bic = LassoLarsIC(criterion='bic', normalize=False)\n",
        "t1 = time.time()\n",
        "model_bic.fit(X, y)\n",
        "t_bic = time.time() - t1\n",
        "alpha_bic_ = model_bic.alpha_\n",
        "\n",
        "model_aic = LassoLarsIC(criterion='aic', normalize=False)\n",
        "model_aic.fit(X, y)\n",
        "alpha_aic_ = model_aic.alpha_\n",
        "\n",
        "\n",
        "def plot_ic_criterion(model, name, color):\n",
        "    criterion_ = model.criterion_\n",
        "    plt.semilogx(model.alphas_ + EPSILON, criterion_, '--', color=color,\n",
        "                 linewidth=3, label='%s criterion' % name)\n",
        "    plt.axvline(model.alpha_ + EPSILON, color=color, linewidth=3,\n",
        "                label='alpha: %s estimate' % name)\n",
        "    plt.xlabel(r'$\\alpha$')\n",
        "    plt.ylabel('criterion')\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "plot_ic_criterion(model_aic, 'AIC', 'b')\n",
        "plot_ic_criterion(model_bic, 'BIC', 'r')\n",
        "plt.legend()\n",
        "plt.title('Information-criterion for model selection (training time %.3fs)'\n",
        "          % t_bic)\n",
        "\n",
        "# #############################################################################\n",
        "# LassoCV: coordinate descent\n",
        "\n",
        "# Compute paths\n",
        "print(\"Computing regularization path using the coordinate descent lasso...\")\n",
        "t1 = time.time()\n",
        "model = LassoCV(cv=20).fit(X, y)\n",
        "t_lasso_cv = time.time() - t1\n",
        "\n",
        "# Display results\n",
        "plt.figure()\n",
        "ymin, ymax = 2300, 3800\n",
        "plt.semilogx(model.alphas_ + EPSILON, model.mse_path_, ':')\n",
        "plt.plot(model.alphas_ + EPSILON, model.mse_path_.mean(axis=-1), 'k',\n",
        "         label='Average across the folds', linewidth=2)\n",
        "plt.axvline(model.alpha_ + EPSILON, linestyle='--', color='k',\n",
        "            label='alpha: CV estimate')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.xlabel(r'$\\alpha$')\n",
        "plt.ylabel('Mean square error')\n",
        "plt.title('Mean square error on each fold: coordinate descent '\n",
        "          '(train time: %.2fs)' % t_lasso_cv)\n",
        "plt.axis('tight')\n",
        "plt.ylim(ymin, ymax)\n",
        "\n",
        "# #############################################################################\n",
        "# LassoLarsCV: least angle regression\n",
        "\n",
        "# Compute paths\n",
        "print(\"Computing regularization path using the Lars lasso...\")\n",
        "t1 = time.time()\n",
        "model = LassoLarsCV(cv=20, normalize=False).fit(X, y)\n",
        "t_lasso_lars_cv = time.time() - t1\n",
        "\n",
        "# Display results\n",
        "plt.figure()\n",
        "plt.semilogx(model.cv_alphas_ + EPSILON, model.mse_path_, ':')\n",
        "plt.semilogx(model.cv_alphas_ + EPSILON, model.mse_path_.mean(axis=-1), 'k',\n",
        "             label='Average across the folds', linewidth=2)\n",
        "plt.axvline(model.alpha_, linestyle='--', color='k',\n",
        "            label='alpha CV')\n",
        "plt.legend()\n",
        "\n",
        "plt.xlabel(r'$\\alpha$')\n",
        "plt.ylabel('Mean square error')\n",
        "plt.title('Mean square error on each fold: Lars (train time: %.2fs)'\n",
        "          % t_lasso_lars_cv)\n",
        "plt.axis('tight')\n",
        "plt.ylim(ymin, ymax)\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMhfi0Dv0nwYeL9Z/N9w5er",
      "include_colab_link": true,
      "name": "Week2_Linear Models_1.1.3. Lasso",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
