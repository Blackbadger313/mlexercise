{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeuIB9Bh86fQ"
      },
      "source": [
        "Poisson regression and non-normal loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1ySOGWL87UH",
        "outputId": "e819a676-ac35-4e9b-db31-ec5ce7b30565"
      },
      "outputs": [],
      "source": [
        "print(__doc__)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "kPWgx-Z68-GG",
        "outputId": "79529b71-36af-44bc-ab25-b35bf4271606"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "\n",
        "df = fetch_openml(data_id=41214, as_frame=True).frame\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "ZnMfd-z39CLn",
        "outputId": "98bbbeaa-7758-4f1c-ecad-c574d3218105"
      },
      "outputs": [],
      "source": [
        "df[\"Frequency\"] = df[\"ClaimNb\"] / df[\"Exposure\"]\n",
        "\n",
        "print(\"Average Frequency = {}\"\n",
        "      .format(np.average(df[\"Frequency\"], weights=df[\"Exposure\"])))\n",
        "\n",
        "print(\"Fraction of exposure with zero claims = {0:.1%}\"\n",
        "      .format(df.loc[df[\"ClaimNb\"] == 0, \"Exposure\"].sum() /\n",
        "              df[\"Exposure\"].sum()))\n",
        "\n",
        "fig, (ax0, ax1, ax2) = plt.subplots(ncols=3, figsize=(16, 4))\n",
        "ax0.set_title(\"Number of claims\")\n",
        "_ = df[\"ClaimNb\"].hist(bins=30, log=True, ax=ax0)\n",
        "ax1.set_title(\"Exposure in years\")\n",
        "_ = df[\"Exposure\"].hist(bins=30, log=True, ax=ax1)\n",
        "ax2.set_title(\"Frequency (number of claims per year)\")\n",
        "_ = df[\"Frequency\"].hist(bins=30, log=True, ax=ax2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2OR14N3b9Lih"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler, KBinsDiscretizer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "\n",
        "log_scale_transformer = make_pipeline(\n",
        "    FunctionTransformer(np.log, validate=False),\n",
        "    StandardScaler()\n",
        ")\n",
        "\n",
        "linear_model_preprocessor = ColumnTransformer(\n",
        "    [\n",
        "        (\"passthrough_numeric\", \"passthrough\",\n",
        "            [\"BonusMalus\"]),\n",
        "        (\"binned_numeric\", KBinsDiscretizer(n_bins=10),\n",
        "            [\"VehAge\", \"DrivAge\"]),\n",
        "        (\"log_scaled_numeric\", log_scale_transformer,\n",
        "            [\"Density\"]),\n",
        "        (\"onehot_categorical\", OneHotEncoder(),\n",
        "            [\"VehBrand\", \"VehPower\", \"VehGas\", \"Region\", \"Area\"]),\n",
        "    ],\n",
        "    remainder=\"drop\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlg5QzJc9R2C"
      },
      "outputs": [],
      "source": [
        "from sklearn.dummy import DummyRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df_train, df_test = train_test_split(df, test_size=0.33, random_state=0)\n",
        "\n",
        "dummy = Pipeline([\n",
        "    (\"preprocessor\", linear_model_preprocessor),\n",
        "    (\"regressor\", DummyRegressor(strategy='mean')),\n",
        "]).fit(df_train, df_train[\"Frequency\"],\n",
        "       regressor__sample_weight=df_train[\"Exposure\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rvjkbkB9VE6",
        "outputId": "85e8884a-5b83-4ddd-84ce-6130113bdd44"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_poisson_deviance\n",
        "\n",
        "\n",
        "def score_estimator(estimator, df_test):\n",
        "    \"\"\"Score an estimator on the test set.\"\"\"\n",
        "    y_pred = estimator.predict(df_test)\n",
        "\n",
        "    print(\"MSE: %.3f\" %\n",
        "          mean_squared_error(df_test[\"Frequency\"], y_pred,\n",
        "                             sample_weight=df_test[\"Exposure\"]))\n",
        "    print(\"MAE: %.3f\" %\n",
        "          mean_absolute_error(df_test[\"Frequency\"], y_pred,\n",
        "                              sample_weight=df_test[\"Exposure\"]))\n",
        "\n",
        "    # Ignore non-positive predictions, as they are invalid for\n",
        "    # the Poisson deviance.\n",
        "    mask = y_pred > 0\n",
        "    if (~mask).any():\n",
        "        n_masked, n_samples = (~mask).sum(), mask.shape[0]\n",
        "        print(f\"WARNING: Estimator yields invalid, non-positive predictions \"\n",
        "              f\" for {n_masked} samples out of {n_samples}. These predictions \"\n",
        "              f\"are ignored when computing the Poisson deviance.\")\n",
        "\n",
        "    print(\"mean Poisson deviance: %.3f\" %\n",
        "          mean_poisson_deviance(df_test[\"Frequency\"][mask],\n",
        "                                y_pred[mask],\n",
        "                                sample_weight=df_test[\"Exposure\"][mask]))\n",
        "\n",
        "\n",
        "print(\"Constant mean frequency evaluation:\")\n",
        "score_estimator(dummy, df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xICGVlqD9X0Q"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "\n",
        "ridge_glm = Pipeline([\n",
        "    (\"preprocessor\", linear_model_preprocessor),\n",
        "    (\"regressor\", Ridge(alpha=1e-6)),\n",
        "]).fit(df_train, df_train[\"Frequency\"],\n",
        "       regressor__sample_weight=df_train[\"Exposure\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcjTQ77e9aOW",
        "outputId": "49b7bf54-64fb-4c25-9aba-d00d743d177c"
      },
      "outputs": [],
      "source": [
        "print(\"Ridge evaluation:\")\n",
        "score_estimator(ridge_glm, df_test)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNmPGwAYefLvrZLayNbrMPL",
      "include_colab_link": true,
      "name": "Week2_Linear Models_1.1.12. Generalized Linear Regression",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
