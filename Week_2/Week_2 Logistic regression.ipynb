{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yR657pVB770i"
      },
      "source": [
        "L1 Penalty and Sparsity in Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 665
        },
        "id": "9ciyF7T17fEs",
        "outputId": "e70a1a32-009c-4f27-8104-6f5fb5d438c8"
      },
      "outputs": [],
      "source": [
        "print(__doc__)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X, y = datasets.load_digits(return_X_y=True)\n",
        "\n",
        "X = StandardScaler().fit_transform(X)\n",
        "\n",
        "# classify small against large digits\n",
        "y = (y > 4).astype(int)\n",
        "\n",
        "l1_ratio = 0.5  # L1 weight in the Elastic-Net regularization\n",
        "\n",
        "fig, axes = plt.subplots(3, 3)\n",
        "\n",
        "# Set regularization parameter\n",
        "for i, (C, axes_row) in enumerate(zip((1, 0.1, 0.01), axes)):\n",
        "    # turn down tolerance for short training time\n",
        "    clf_l1_LR = LogisticRegression(C=C, penalty='l1', tol=0.01, solver='saga')\n",
        "    clf_l2_LR = LogisticRegression(C=C, penalty='l2', tol=0.01, solver='saga')\n",
        "    clf_en_LR = LogisticRegression(C=C, penalty='elasticnet', solver='saga',\n",
        "                                   l1_ratio=l1_ratio, tol=0.01)\n",
        "    clf_l1_LR.fit(X, y)\n",
        "    clf_l2_LR.fit(X, y)\n",
        "    clf_en_LR.fit(X, y)\n",
        "\n",
        "    coef_l1_LR = clf_l1_LR.coef_.ravel()\n",
        "    coef_l2_LR = clf_l2_LR.coef_.ravel()\n",
        "    coef_en_LR = clf_en_LR.coef_.ravel()\n",
        "\n",
        "    # coef_l1_LR contains zeros due to the\n",
        "    # L1 sparsity inducing norm\n",
        "\n",
        "    sparsity_l1_LR = np.mean(coef_l1_LR == 0) * 100\n",
        "    sparsity_l2_LR = np.mean(coef_l2_LR == 0) * 100\n",
        "    sparsity_en_LR = np.mean(coef_en_LR == 0) * 100\n",
        "\n",
        "    print(\"C=%.2f\" % C)\n",
        "    print(\"{:<40} {:.2f}%\".format(\"Sparsity with L1 penalty:\", sparsity_l1_LR))\n",
        "    print(\"{:<40} {:.2f}%\".format(\"Sparsity with Elastic-Net penalty:\",\n",
        "                                  sparsity_en_LR))\n",
        "    print(\"{:<40} {:.2f}%\".format(\"Sparsity with L2 penalty:\", sparsity_l2_LR))\n",
        "    print(\"{:<40} {:.2f}\".format(\"Score with L1 penalty:\",\n",
        "                                 clf_l1_LR.score(X, y)))\n",
        "    print(\"{:<40} {:.2f}\".format(\"Score with Elastic-Net penalty:\",\n",
        "                                 clf_en_LR.score(X, y)))\n",
        "    print(\"{:<40} {:.2f}\".format(\"Score with L2 penalty:\",\n",
        "                                 clf_l2_LR.score(X, y)))\n",
        "\n",
        "    if i == 0:\n",
        "        axes_row[0].set_title(\"L1 penalty\")\n",
        "        axes_row[1].set_title(\"Elastic-Net\\nl1_ratio = %s\" % l1_ratio)\n",
        "        axes_row[2].set_title(\"L2 penalty\")\n",
        "\n",
        "    for ax, coefs in zip(axes_row, [coef_l1_LR, coef_en_LR, coef_l2_LR]):\n",
        "        ax.imshow(np.abs(coefs.reshape(8, 8)), interpolation='nearest',\n",
        "                  cmap='binary', vmax=1, vmin=0)\n",
        "        ax.set_xticks(())\n",
        "        ax.set_yticks(())\n",
        "\n",
        "    axes_row[0].set_ylabel('C = %s' % C)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhAnU1hb8Aqs"
      },
      "source": [
        "Regularization path of L1- Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "mU11a9G88BGh",
        "outputId": "70145f41-7518-44df-c291-4f9a43f3d4ab"
      },
      "outputs": [],
      "source": [
        "print(__doc__)\n",
        "\n",
        "from time import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import linear_model\n",
        "from sklearn import datasets\n",
        "from sklearn.svm import l1_min_c\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X = X[y != 2]\n",
        "y = y[y != 2]\n",
        "\n",
        "X /= X.max()  # Normalize X to speed-up convergence\n",
        "\n",
        "# #############################################################################\n",
        "# Demo path functions\n",
        "\n",
        "cs = l1_min_c(X, y, loss='log') * np.logspace(0, 7, 16)\n",
        "\n",
        "\n",
        "print(\"Computing regularization path ...\")\n",
        "start = time()\n",
        "clf = linear_model.LogisticRegression(penalty='l1', solver='liblinear',\n",
        "                                      tol=1e-6, max_iter=int(1e6),\n",
        "                                      warm_start=True,\n",
        "                                      intercept_scaling=10000.)\n",
        "coefs_ = []\n",
        "for c in cs:\n",
        "    clf.set_params(C=c)\n",
        "    clf.fit(X, y)\n",
        "    coefs_.append(clf.coef_.ravel().copy())\n",
        "print(\"This took %0.3fs\" % (time() - start))\n",
        "\n",
        "coefs_ = np.array(coefs_)\n",
        "plt.plot(np.log10(cs), coefs_, marker='o')\n",
        "ymin, ymax = plt.ylim()\n",
        "plt.xlabel('log(C)')\n",
        "plt.ylabel('Coefficients')\n",
        "plt.title('Logistic Regression Path')\n",
        "plt.axis('tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDJ-5d7D8JFM"
      },
      "source": [
        "Plot multinomial and One-vs-Rest Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "JUFL1Zk48M3X",
        "outputId": "b8482b0d-3878-41c7-b1c2-debbc0983bb3"
      },
      "outputs": [],
      "source": [
        "print(__doc__)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# make 3-class dataset for classification\n",
        "centers = [[-5, 0], [0, 1.5], [5, -1]]\n",
        "X, y = make_blobs(n_samples=1000, centers=centers, random_state=40)\n",
        "transformation = [[0.4, 0.2], [-0.4, 1.2]]\n",
        "X = np.dot(X, transformation)\n",
        "\n",
        "for multi_class in ('multinomial', 'ovr'):\n",
        "    clf = LogisticRegression(solver='sag', max_iter=100, random_state=42,\n",
        "                             multi_class=multi_class).fit(X, y)\n",
        "\n",
        "    # print the training scores\n",
        "    print(\"training score : %.3f (%s)\" % (clf.score(X, y), multi_class))\n",
        "\n",
        "    # create a mesh to plot in\n",
        "    h = .02  # step size in the mesh\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "\n",
        "    # Plot the decision boundary. For that, we will assign a color to each\n",
        "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
        "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    # Put the result into a color plot\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.figure()\n",
        "    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n",
        "    plt.title(\"Decision surface of LogisticRegression (%s)\" % multi_class)\n",
        "    plt.axis('tight')\n",
        "\n",
        "    # Plot also the training points\n",
        "    colors = \"bry\"\n",
        "    for i, color in zip(clf.classes_, colors):\n",
        "        idx = np.where(y == i)\n",
        "        plt.scatter(X[idx, 0], X[idx, 1], c=color, cmap=plt.cm.Paired,\n",
        "                    edgecolor='black', s=20)\n",
        "\n",
        "    # Plot the three one-against-all classifiers\n",
        "    xmin, xmax = plt.xlim()\n",
        "    ymin, ymax = plt.ylim()\n",
        "    coef = clf.coef_\n",
        "    intercept = clf.intercept_\n",
        "\n",
        "    def plot_hyperplane(c, color):\n",
        "        def line(x0):\n",
        "            return (-(x0 * coef[c, 0]) - intercept[c]) / coef[c, 1]\n",
        "        plt.plot([xmin, xmax], [line(xmin), line(xmax)],\n",
        "                 ls=\"--\", color=color)\n",
        "\n",
        "    for i, color in zip(clf.classes_, colors):\n",
        "        plot_hyperplane(i, color)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzRsbp_T8Qs1"
      },
      "source": [
        "Multiclass sparse logistic regression on 20newgroups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 736
        },
        "id": "t96Hs0ol8VRZ",
        "outputId": "b7921d1c-364d-4870-b158-12aeb5feee50"
      },
      "outputs": [],
      "source": [
        "import timeit\n",
        "import warnings\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups_vectorized\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "\n",
        "print(__doc__)\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning,\n",
        "                        module=\"sklearn\")\n",
        "t0 = timeit.default_timer()\n",
        "\n",
        "# We use SAGA solver\n",
        "solver = 'saga'\n",
        "\n",
        "# Turn down for faster run time\n",
        "n_samples = 10000\n",
        "\n",
        "X, y = fetch_20newsgroups_vectorized(subset='all', return_X_y=True)\n",
        "X = X[:n_samples]\n",
        "y = y[:n_samples]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    random_state=42,\n",
        "                                                    stratify=y,\n",
        "                                                    test_size=0.1)\n",
        "train_samples, n_features = X_train.shape\n",
        "n_classes = np.unique(y).shape[0]\n",
        "\n",
        "print('Dataset 20newsgroup, train_samples=%i, n_features=%i, n_classes=%i'\n",
        "      % (train_samples, n_features, n_classes))\n",
        "\n",
        "models = {'ovr': {'name': 'One versus Rest', 'iters': [1, 2, 4]},\n",
        "          'multinomial': {'name': 'Multinomial', 'iters': [1, 3, 7]}}\n",
        "\n",
        "for model in models:\n",
        "    # Add initial chance-level values for plotting purpose\n",
        "    accuracies = [1 / n_classes]\n",
        "    times = [0]\n",
        "    densities = [1]\n",
        "\n",
        "    model_params = models[model]\n",
        "\n",
        "    # Small number of epochs for fast runtime\n",
        "    for this_max_iter in model_params['iters']:\n",
        "        print('[model=%s, solver=%s] Number of epochs: %s' %\n",
        "              (model_params['name'], solver, this_max_iter))\n",
        "        lr = LogisticRegression(solver=solver,\n",
        "                                multi_class=model,\n",
        "                                penalty='l1',\n",
        "                                max_iter=this_max_iter,\n",
        "                                random_state=42,\n",
        "                                )\n",
        "        t1 = timeit.default_timer()\n",
        "        lr.fit(X_train, y_train)\n",
        "        train_time = timeit.default_timer() - t1\n",
        "\n",
        "        y_pred = lr.predict(X_test)\n",
        "        accuracy = np.sum(y_pred == y_test) / y_test.shape[0]\n",
        "        density = np.mean(lr.coef_ != 0, axis=1) * 100\n",
        "        accuracies.append(accuracy)\n",
        "        densities.append(density)\n",
        "        times.append(train_time)\n",
        "    models[model]['times'] = times\n",
        "    models[model]['densities'] = densities\n",
        "    models[model]['accuracies'] = accuracies\n",
        "    print('Test accuracy for model %s: %.4f' % (model, accuracies[-1]))\n",
        "    print('%% non-zero coefficients for model %s, '\n",
        "          'per class:\\n %s' % (model, densities[-1]))\n",
        "    print('Run time (%i epochs) for model %s:'\n",
        "          '%.2f' % (model_params['iters'][-1], model, times[-1]))\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "for model in models:\n",
        "    name = models[model]['name']\n",
        "    times = models[model]['times']\n",
        "    accuracies = models[model]['accuracies']\n",
        "    ax.plot(times, accuracies, marker='o',\n",
        "            label='Model: %s' % name)\n",
        "    ax.set_xlabel('Train time (s)')\n",
        "    ax.set_ylabel('Test accuracy')\n",
        "ax.legend()\n",
        "fig.suptitle('Multinomial vs One-vs-Rest Logistic L1\\n'\n",
        "             'Dataset %s' % '20newsgroups')\n",
        "fig.tight_layout()\n",
        "fig.subplots_adjust(top=0.85)\n",
        "run_time = timeit.default_timer() - t0\n",
        "print('Example run in %.3f s' % run_time)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQlg3xTV8c2n"
      },
      "source": [
        "MNIST classification using multinomial logistic + L1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "emztnd1Y8eAh",
        "outputId": "9f27470d-064a-4b9b-ac8b-6c7437940b14"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils import check_random_state\n",
        "\n",
        "print(__doc__)\n",
        "\n",
        "# Turn down for faster convergence\n",
        "t0 = time.time()\n",
        "train_samples = 5000\n",
        "\n",
        "# Load data from https://www.openml.org/d/554\n",
        "X, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)\n",
        "\n",
        "random_state = check_random_state(0)\n",
        "permutation = random_state.permutation(X.shape[0])\n",
        "X = X[permutation]\n",
        "y = y[permutation]\n",
        "X = X.reshape((X.shape[0], -1))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, train_size=train_samples, test_size=10000)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Turn up tolerance for faster convergence\n",
        "clf = LogisticRegression(\n",
        "    C=50. / train_samples, penalty='l1', solver='saga', tol=0.1\n",
        ")\n",
        "clf.fit(X_train, y_train)\n",
        "sparsity = np.mean(clf.coef_ == 0) * 100\n",
        "score = clf.score(X_test, y_test)\n",
        "# print('Best C % .4f' % clf.C_)\n",
        "print(\"Sparsity with L1 penalty: %.2f%%\" % sparsity)\n",
        "print(\"Test score with L1 penalty: %.4f\" % score)\n",
        "\n",
        "coef = clf.coef_.copy()\n",
        "plt.figure(figsize=(10, 5))\n",
        "scale = np.abs(coef).max()\n",
        "for i in range(10):\n",
        "    l1_plot = plt.subplot(2, 5, i + 1)\n",
        "    l1_plot.imshow(coef[i].reshape(28, 28), interpolation='nearest',\n",
        "                   cmap=plt.cm.RdBu, vmin=-scale, vmax=scale)\n",
        "    l1_plot.set_xticks(())\n",
        "    l1_plot.set_yticks(())\n",
        "    l1_plot.set_xlabel('Class %i' % i)\n",
        "plt.suptitle('Classification vector for...')\n",
        "\n",
        "run_time = time.time() - t0\n",
        "print('Example run in %.3f s' % run_time)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPIu5vRbp90CaSF6Pvfakj8",
      "include_colab_link": true,
      "name": "Week2_Linear Models_1.1.11. Logistic regression",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
